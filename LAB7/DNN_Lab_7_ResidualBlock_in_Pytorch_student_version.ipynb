{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gcTwzhX8fBqs"
   },
   "source": [
    "Code based on https://github.com/pytorch/examples/blob/master/mnist/main.py\n",
    "\n",
    "In this exercise, we are going to implement a [ResNet-like](https://arxiv.org/pdf/1512.03385.pdf) architecture for the image classification task.\n",
    "The model is trained on the [MNIST](https://en.wikipedia.org/wiki/MNIST_database) dataset.\n",
    "\n",
    "Tasks:\n",
    "\n",
    "    1. Implement residual connections in the missing places in the code.\n",
    "\n",
    "    2. Check that the given implementation reaches 97% test accuracy after a few epochs.\n",
    "\n",
    "    3. Check that when extending the residual blocks to 20 (having 40+ layers total), the model still trains well, i.e., achieves 97+% accuracy after three epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "IYAsziKffBFV"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "iVlHDDLJzRA7"
   },
   "outputs": [],
   "source": [
    "class ResidualConnection(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv_block_1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=out_channels,\n",
    "                kernel_size=3,\n",
    "                padding=1,\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.conv_block_2 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=out_channels,\n",
    "                out_channels=out_channels,\n",
    "                kernel_size=3,\n",
    "                padding=1,\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        a = self.conv_block_1(x)\n",
    "        a = self.conv_block_2(a)\n",
    "        a += x\n",
    "        return F.relu(a)\n",
    "        ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "rKS1CQxQzRA7"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.rc = nn.Sequential(\n",
    "            ResidualConnection(1, 16),\n",
    "            ResidualConnection(16, 16),\n",
    "            ResidualConnection(16, 16),\n",
    "        )\n",
    "        self.fc = nn.Linear(\n",
    "            28 * 28 * 16, 10\n",
    "        )  # 28 * 28 * 16 is the size of flattened output of the last ResidualConnection\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.rc(x)\n",
    "        x = nn.Flatten(start_dim=1)(x)\n",
    "        x = self.fc(x)\n",
    "        output = nn.LogSoftmax(dim=1)(x)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "DMtap4QCfBH8"
   },
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch, log_interval):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(data),\n",
    "                    len(train_loader.dataset),\n",
    "                    100.0 * batch_idx / len(train_loader),\n",
    "                    loss.item(),\n",
    "                )\n",
    "            )\n",
    "\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(\n",
    "                output, target, reduction=\"sum\"\n",
    "            ).item()  # sum up batch loss\n",
    "            pred = output.argmax(\n",
    "                dim=1, keepdim=True\n",
    "            )  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print(\n",
    "        \"\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n\".format(\n",
    "            test_loss,\n",
    "            correct,\n",
    "            len(test_loader.dataset),\n",
    "            100.0 * correct / len(test_loader.dataset),\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "K5GlMs1-fBKP"
   },
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "test_batch_size = 1000\n",
    "epochs = 3\n",
    "lr = 1e-2\n",
    "use_cuda = False\n",
    "seed = 1\n",
    "log_interval = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "WgfUP23AfBMd"
   },
   "outputs": [],
   "source": [
    "use_cuda = not use_cuda and torch.cuda.is_available()\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "\n",
    "train_kwargs = {\"batch_size\": batch_size}\n",
    "test_kwargs = {\"batch_size\": test_batch_size}\n",
    "if use_cuda:\n",
    "    cuda_kwargs = {\"num_workers\": 1, \"pin_memory\": True, \"shuffle\": True}\n",
    "    train_kwargs.update(cuda_kwargs)\n",
    "    test_kwargs.update(cuda_kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "o0KPoUtsfBOs"
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    ")\n",
    "dataset1 = datasets.MNIST(\"../data\", train=True, download=True, transform=transform)\n",
    "dataset2 = datasets.MNIST(\"../data\", train=False, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(dataset1, **train_kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ezvIQbgsfBRT",
    "outputId": "3f6621ef-0bad-46c6-bd8f-ac535db8e9af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.619462\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: inf\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 2.873294\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 1.624378\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.911324\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.978073\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.565717\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.389398\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.474406\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.258957\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.249693\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.330889\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.220970\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.201127\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.314271\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.172623\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.193273\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.147164\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.240124\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.075754\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.120161\n",
      "Train Epoch: 1 [53760/60000 (89%)]\tLoss: 0.128105\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.121059\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.007742\n",
      "\n",
      "Test set: Average loss: 0.1308, Accuracy: 9653/10000 (97%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.184813\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.151525\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.152502\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.080043\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.078810\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.090847\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.136279\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.133324\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.135693\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.085254\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.137194\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.138378\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.087675\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.151778\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.131271\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.065262\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.100235\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.057522\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.115673\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.041764\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.065639\n",
      "Train Epoch: 2 [53760/60000 (89%)]\tLoss: 0.042096\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.081719\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.001868\n",
      "\n",
      "Test set: Average loss: 0.0807, Accuracy: 9763/10000 (98%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.118108\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.150668\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.093953\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.030192\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.046424\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.064410\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.089840\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.060229\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.086572\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.059089\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.084621\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.081421\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.073538\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.071636\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.102275\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.061391\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.068084\n",
      "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.030922\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.057236\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.028764\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.047481\n",
      "Train Epoch: 3 [53760/60000 (89%)]\tLoss: 0.037824\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.036992\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.001257\n",
      "\n",
      "Test set: Average loss: 0.0701, Accuracy: 9797/10000 (98%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Net().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(model, device, train_loader, optimizer, epoch, log_interval)\n",
    "    test(model, device, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "6fa2fa4f4d9d3d9ca73eb3739cc0e85a72773041ed8c7376d5dc2c41e6946bf8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
